{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ab2e4b",
   "metadata": {},
   "source": [
    "# INTRO\n",
    "\n",
    "Для заказчика необходимо найти решение определения уровня сложности английского языка в фильмах по субтитрам.\n",
    "\n",
    "Заказчик предоставил данные:\n",
    "- английские словари Oxford, в которых слова распределены по уровню сложности\n",
    "- набор файлов-субтитров, рассортированных по каталогам в соответствии с уровнем сложности\n",
    "- excel файл со список несортированных фильмов и указанием их уровня\n",
    "\n",
    "Цель:\n",
    "- создание модели определения уровня сложности\n",
    "\n",
    "Задачи:\n",
    "- изучение и обработка предоставленного материала\n",
    "- обработка текста субтитров и подготовка для машинного обучения\n",
    "- расчет и добавление дополнительных признаков\n",
    "- тестирование модели и подбор гиперпараметров\n",
    "- выделение наиболее удачных признаков\n",
    "- обучение оптимальной модели на датасете с наиболее полезными признаками\n",
    "- создание приложения на платформе streamlit для демонстрации решения\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16cd5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, mean_squared_error, f1_score\n",
    "\n",
    "import xgboost\n",
    "import catboost\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    from spacy.lang.en import stop_words\n",
    "except:\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en\n",
    "    import spacy\n",
    "    from spacy.lang.en import stop_words\n",
    "\n",
    "try:\n",
    "    import chardet\n",
    "except:\n",
    "    !pip install chardet\n",
    "    import chardet\n",
    "\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "except:\n",
    "    !pip install pypdf\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "try:\n",
    "    import pysrt\n",
    "except:\n",
    "    !pip install pysrt\n",
    "    import pysrt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da1824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('ggplot')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "SCORES_PATH    = 'English_level/English_scores'\n",
    "SUBTITLES_PATH = 'English_level/English_scores/Subtitles_all'\n",
    "OXFORD_PATH    = 'English_level/Oxford_CEFR_level'\n",
    "\n",
    "ENGLISH_LEVELS = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "\n",
    "RANDOM_STATE = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f98292",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99487174",
   "metadata": {},
   "source": [
    "## The Oxford core by CEFR level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e3d58",
   "metadata": {},
   "source": [
    "В словарях Oxford 3000 и 5000 содержатся наиболее важные слова, которые должен знать каждый, кто учит английский.\n",
    "\n",
    "Словарь Oxford соответствует стандартам CEFR – The Common European Framework of Reference for Languages. Это общеевропейские компетенции владения иностранным языком: изучение, преподавание, оценка — система уровней владения иностранным языком, используемая в Европейском Союзе.\n",
    "\n",
    "У нас словари представлены в виде pdf-файлов.\n",
    "- прочитаем их,\n",
    "- разобьем на строки,\n",
    "- выделим слова,\n",
    "- определим уровень\n",
    "- сохраним, из какого словаря взяли слово\n",
    "- удалим дубликаты, оставим те случаи, где слову присвоена более высокая сложность\n",
    "- выгрузим в joblib для дальнейшего использования в приложении Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00b3d3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American_Oxford_3000_by_CEFR_level.pdf\n",
      "The_Oxford_3000_by_CEFR_level.pdf\n",
      "The_Oxford_5000_by_CEFR_level.pdf\n",
      "American_Oxford_5000_by_CEFR_level.pdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c5b40\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c5b40_level0_col0\" class=\"col_heading level0 col0\" >word</th>\n",
       "      <th id=\"T_c5b40_level0_col1\" class=\"col_heading level0 col1\" >source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >level</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c5b40_level0_row0\" class=\"row_heading level0 row0\" >A1</th>\n",
       "      <td id=\"T_c5b40_row0_col0\" class=\"data row0 col0\" >735 слов</td>\n",
       "      <td id=\"T_c5b40_row0_col1\" class=\"data row0 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5b40_level0_row1\" class=\"row_heading level0 row1\" >A2</th>\n",
       "      <td id=\"T_c5b40_row1_col0\" class=\"data row1 col0\" >747 слов</td>\n",
       "      <td id=\"T_c5b40_row1_col1\" class=\"data row1 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5b40_level0_row2\" class=\"row_heading level0 row2\" >B1</th>\n",
       "      <td id=\"T_c5b40_row2_col0\" class=\"data row2 col0\" >758 слов</td>\n",
       "      <td id=\"T_c5b40_row2_col1\" class=\"data row2 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5b40_level0_row3\" class=\"row_heading level0 row3\" >B2</th>\n",
       "      <td id=\"T_c5b40_row3_col0\" class=\"data row3 col0\" >1,446 слов</td>\n",
       "      <td id=\"T_c5b40_row3_col1\" class=\"data row3 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5b40_level0_row4\" class=\"row_heading level0 row4\" >C1</th>\n",
       "      <td id=\"T_c5b40_row4_col0\" class=\"data row4 col0\" >1,198 слов</td>\n",
       "      <td id=\"T_c5b40_row4_col1\" class=\"data row4 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x103cd5ca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_pdf(content):\n",
    "    \n",
    "    current_level = ''\n",
    "    words_by_level = {key:[] for key in ENGLISH_LEVELS}\n",
    "    \n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "\n",
    "        if 'Oxford' in line or 'English' in line:\n",
    "            pass\n",
    "        \n",
    "        elif line in ENGLISH_LEVELS:\n",
    "            level = line\n",
    "            \n",
    "        elif ' ' in line:\n",
    "            line = re.sub(r'\\d|,', '', line.lower())   # remove digits and commas and transform to lowercase\n",
    "            words_by_level[level] += [line.split()[0]] #get the first occurency in line as word\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return words_by_level\n",
    "\n",
    "\n",
    "content = []\n",
    "oxford_words = pd.DataFrame(columns=['word', 'level', 'source', 'type'])\n",
    "\n",
    "# load all pdfs\n",
    "for dirname, _, filenames in os.walk(OXFORD_PATH):\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        reader = PdfReader(f'{OXFORD_PATH}/{filename}')\n",
    "        for page in reader.pages:\n",
    "            content += page.extract_text().splitlines()\n",
    "        processed = process_pdf(content)\n",
    "        for level in processed:\n",
    "            dict_type = 'us' if 'American' in filename else 'uk'\n",
    "            oxford_words = pd.concat([oxford_words,\n",
    "                                      pd.DataFrame({'word'   : processed[level],\n",
    "                                                    'level'  : level,\n",
    "                                                    'source' : filename,\n",
    "                                                    'type'   : dict_type\n",
    "                                                   })\n",
    "                                     ])\n",
    "\n",
    "# sort words and keep unique with higher levels\n",
    "oxford_words = oxford_words.sort_values(by=['word', 'level'], ascending=True)\n",
    "oxford_words = oxford_words.drop_duplicates(subset=['word'], keep='last')\n",
    "\n",
    "# после сортировки и удаления дубликатов остался только американский инглиш\n",
    "# похоже там все уровни слов считаются выше\n",
    "\n",
    "oxford_words.groupby('level') \\\n",
    "            .agg({'word':'count', 'source': 'unique'}) \\\n",
    "            .style.format({'word':'{:,.0f} слов'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22134c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english_level_oxford.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(oxford_words,'english_level_oxford.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d6b2f",
   "metadata": {},
   "source": [
    "## Subtitles .srt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ea637",
   "metadata": {},
   "source": [
    "Субтитры у нас представлены в виде файлов .srt – это текстовые файлы, в которых информация представлены в таком виде:\n",
    "\n",
    "        18\n",
    "        00:04:59,796 --> 00:04:59,921\n",
    "        - ( police radio chatter )\n",
    "        - Man: What's the difference\n",
    "    \n",
    "То есть номер реплики, время и текст.\n",
    "\n",
    "Файлы рассортированы по папкам, соответствующим уровням сложности языка..\n",
    "\n",
    "Просканируем все файлы построчно:\n",
    "- удалим из текста по возможности тэги, скобки, указания говорящего лица\n",
    "- нетекстовые знаки, \n",
    "- лишние пробелы и переводы строки\n",
    "\n",
    "Сохраним временные характеристики реплик:\n",
    "- длительность реплики\n",
    "- скорость речи в знаках в секунду\n",
    "- скорость речи в словах в секунду\n",
    "\n",
    "Соберем все в виде датасета для дальнейшей обработки.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9555659b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "посторонний файл .DS_Store\n",
      "посторонний файл .DS_Store\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>duration</th>\n",
       "      <th>charsrate</th>\n",
       "      <th>wordsrate</th>\n",
       "      <th>level</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "      <th>A1ratio</th>\n",
       "      <th>A2ratio</th>\n",
       "      <th>B1ratio</th>\n",
       "      <th>B2ratio</th>\n",
       "      <th>C1ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crown, The S01E01 - Wolferton Splash.en</td>\n",
       "      <td>in seeking his british nationalization, his ro...</td>\n",
       "      <td>[00:00:04.560000, 00:00:03.400000, 00:00:04.28...</td>\n",
       "      <td>[12.500000000000002, 11.176470588235295, 10.74...</td>\n",
       "      <td>[1.7543859649122808, 2.058823529411765, 1.4018...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suits.Episode 1- Denial</td>\n",
       "      <td>you're the most amazing woman i have ever met....</td>\n",
       "      <td>[00:00:02.034000, 00:00:04.069000, 00:00:01.90...</td>\n",
       "      <td>[22.123893805309738, 9.584664536741213, 11.572...</td>\n",
       "      <td>[4.424778761061947, 1.9660850331776847, 1.5781...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...</td>\n",
       "      <td>i've been after sutter for three years now. t...</td>\n",
       "      <td>[00:00:01.460000, 00:00:01.210000, 00:00:01.46...</td>\n",
       "      <td>[10.95890410958904, 34.710743801652896, 26.027...</td>\n",
       "      <td>[0.0, 6.6115702479338845, 5.47945205479452, 2....</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suits.S02E08.HDTV.x264-EVOLVE</td>\n",
       "      <td>you're late. nope. 30 seconds early. good. le...</td>\n",
       "      <td>[00:00:02.001000, 00:00:01.033000, 00:00:01.13...</td>\n",
       "      <td>[8.495752123938031, 11.616650532429817, 20.300...</td>\n",
       "      <td>[0.0, 1.936108422071636, 3.53045013239188, 2.7...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE</td>\n",
       "      <td>are you sure i can't convince you to stay? no....</td>\n",
       "      <td>[00:00:02.710000, 00:00:01, 00:00:02.170000, 0...</td>\n",
       "      <td>[15.498154981549815, 3.0, 19.81566820276498, 1...</td>\n",
       "      <td>[3.3210332103321036, 1.0, 4.608294930875576, 3...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0            Crown, The S01E01 - Wolferton Splash.en   \n",
       "1                            Suits.Episode 1- Denial   \n",
       "2  Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...   \n",
       "3                      Suits.S02E08.HDTV.x264-EVOLVE   \n",
       "4  Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE   \n",
       "\n",
       "                                             content  \\\n",
       "0  in seeking his british nationalization, his ro...   \n",
       "1  you're the most amazing woman i have ever met....   \n",
       "2   i've been after sutter for three years now. t...   \n",
       "3   you're late. nope. 30 seconds early. good. le...   \n",
       "4  are you sure i can't convince you to stay? no....   \n",
       "\n",
       "                                            duration  \\\n",
       "0  [00:00:04.560000, 00:00:03.400000, 00:00:04.28...   \n",
       "1  [00:00:02.034000, 00:00:04.069000, 00:00:01.90...   \n",
       "2  [00:00:01.460000, 00:00:01.210000, 00:00:01.46...   \n",
       "3  [00:00:02.001000, 00:00:01.033000, 00:00:01.13...   \n",
       "4  [00:00:02.710000, 00:00:01, 00:00:02.170000, 0...   \n",
       "\n",
       "                                           charsrate  \\\n",
       "0  [12.500000000000002, 11.176470588235295, 10.74...   \n",
       "1  [22.123893805309738, 9.584664536741213, 11.572...   \n",
       "2  [10.95890410958904, 34.710743801652896, 26.027...   \n",
       "3  [8.495752123938031, 11.616650532429817, 20.300...   \n",
       "4  [15.498154981549815, 3.0, 19.81566820276498, 1...   \n",
       "\n",
       "                                           wordsrate level  lemmas  A1  A2  \\\n",
       "0  [1.7543859649122808, 2.058823529411765, 1.4018...    B2     NaN NaN NaN   \n",
       "1  [4.424778761061947, 1.9660850331776847, 1.5781...    B2     NaN NaN NaN   \n",
       "2  [0.0, 6.6115702479338845, 5.47945205479452, 2....    B2     NaN NaN NaN   \n",
       "3  [0.0, 1.936108422071636, 3.53045013239188, 2.7...    B2     NaN NaN NaN   \n",
       "4  [3.3210332103321036, 1.0, 4.608294930875576, 3...    B2     NaN NaN NaN   \n",
       "\n",
       "   B1  B2  C1  A1ratio  A2ratio  B1ratio  B2ratio  C1ratio  \n",
       "0 NaN NaN NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "1 NaN NaN NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "2 NaN NaN NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "3 NaN NaN NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "4 NaN NaN NaN      NaN      NaN      NaN      NaN      NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process line\n",
    "def process_line(line):\n",
    "    if re.search(r'[A-Za-z]',line): \n",
    "        line = line.lower()\n",
    "        line = re.sub(r'\\n', ' ', line)                            # remove new lines\n",
    "        line = re.sub(r'- ', ' ', line)                            # remove dash\n",
    "        line = re.sub(r'\\<[^\\<]+?\\>', '', line)                    # remove html tags\n",
    "        line = re.sub(r'\\([^\\(]+?\\)', '', line)                    # remove () parenthesis\n",
    "        line = re.sub(r'\\[[^\\[]+?\\]', '', line)                    # remove [] parenthesis\n",
    "        line = re.sub(r'^([\\w#\\s]+\\:)', ' ', line)                 # remove speaker tag\n",
    "        line = re.sub(r'[^[:alnum:][:punct:][:blank:]]',' ', line) # remove all other non-speach shars\n",
    "        line = re.sub(r'\\s\\s+', ' ', line).strip()                 # remove extra spaces\n",
    "    return line\n",
    "\n",
    "\n",
    "# process text line by line   \n",
    "def process_text(content):\n",
    "    text = []\n",
    "    duration  = []\n",
    "    charsrate = []\n",
    "    wordsrate  = []\n",
    "    for item in content:\n",
    "        if not hasattr(item, 'duration'):\n",
    "            print('no')\n",
    "        if item.duration.ordinal>0:\n",
    "            line = process_line(item.text_without_tags)\n",
    "            text.append(line)\n",
    "            duration.append(item.duration.to_time())\n",
    "            charsrate.append(item.characters_per_second)\n",
    "            wordsrate.append(len(line.split())/item.duration.ordinal*1000.0)\n",
    "    return ' '.join(text), duration, charsrate, wordsrate\n",
    "\n",
    "\n",
    "# process file\n",
    "def process_srt(dirname, filename):\n",
    "    global count\n",
    "    if not filename.endswith('.srt'):                        # skip non srt files\n",
    "        print('посторонний файл', filename)\n",
    "        return False\n",
    "    fullpath = os.path.join(dirname,filename)\n",
    "    try:\n",
    "        enc = chardet.detect(open(fullpath, \"rb\").read())['encoding']\n",
    "        content = pysrt.open(fullpath, encoding=enc)\n",
    "    except:\n",
    "        print('не прочиталось', filename)\n",
    "        return False\n",
    "    return process_text(content)                            # clean text and return\n",
    "\n",
    "\n",
    "# movies dataset template\n",
    "movies = pd.DataFrame(columns=['filename', \n",
    "                               'content', \n",
    "                               'duration', \n",
    "                               'charsrate', \n",
    "                               'wordsrate', \n",
    "                               'level', \n",
    "                               'lemmas'] + ENGLISH_LEVELS + [l+'ratio' for l in ENGLISH_LEVELS]\n",
    "                     )\n",
    "\n",
    "# recursive walk through dirs\n",
    "for dirname, _, filenames in os.walk(SUBTITLES_PATH):\n",
    "    for filename in filenames:\n",
    "        level  = dirname.split('/')[-1]                   # get level name from dir\n",
    "        result = process_srt(dirname, filename)           # process file\n",
    "        if result:                                        # add movie to dataframe\n",
    "            subs, duration, charsrate, wordsrate = result\n",
    "            movies.loc[len(movies)] = \\\n",
    "                {'filename' : filename.replace('.srt', ''),\n",
    "                 'content'  : subs,\n",
    "                 'duration' : duration, \n",
    "                 'charsrate': charsrate, \n",
    "                 'wordsrate': wordsrate, \n",
    "                 'level'    : level\n",
    "                }\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a291efc",
   "metadata": {},
   "source": [
    "## Excel labels processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6a507",
   "metadata": {},
   "source": [
    "Часть файлов лежит в общем каталоге без указания уровня сложности. Для них заказчик предоставил excel-файл со списком фильмов и уровнями.\n",
    " \n",
    "Загрузим файл и изучим его:\n",
    "- удалим дубликаты\n",
    "- поправим несовпадения названий\n",
    "- из указанных уровней оставим более сложный вариант\n",
    "- удалим фильмы, для которых нет информации по уровню сложности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e329ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 241 entries, 0 to 240\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Movie   241 non-null    object\n",
      " 1   Level   241 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ KB\n",
      "\n",
      "Дубликаты полные 2\n",
      "Дубликаты в названиях фильмов 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               movie   level\n",
       "id                                          \n",
       "0          10_Cloverfield_lane(2016)      B1\n",
       "1   10_things_I_hate_about_you(1999)      B1\n",
       "2               A_knights_tale(2001)      B2\n",
       "3               A_star_is_born(2018)      B2\n",
       "4                      Aladdin(1992)  A2/A2+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_labels = pd.read_excel(f'{SCORES_PATH}/movies_labels.xlsx', index_col='id')\n",
    "movie_labels.info()\n",
    "movie_labels.columns = ['movie', 'level']\n",
    "print('\\nДубликаты полные', movie_labels.duplicated().sum())\n",
    "print('Дубликаты в названиях фильмов', movie_labels.movie.duplicated().sum())\n",
    "movie_labels = movie_labels.drop_duplicates()\n",
    "movie_labels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0885dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cadf9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cadf9_level0_col0\" class=\"col_heading level0 col0\" >movie</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >level</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cadf9_level0_row0\" class=\"row_heading level0 row0\" >A2</th>\n",
       "      <td id=\"T_cadf9_row0_col0\" class=\"data row0 col0\" >32 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cadf9_level0_row1\" class=\"row_heading level0 row1\" >B1</th>\n",
       "      <td id=\"T_cadf9_row1_col0\" class=\"data row1 col0\" >58 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cadf9_level0_row2\" class=\"row_heading level0 row2\" >B2</th>\n",
       "      <td id=\"T_cadf9_row2_col0\" class=\"data row2 col0\" >109 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cadf9_level0_row3\" class=\"row_heading level0 row3\" >C1</th>\n",
       "      <td id=\"T_cadf9_row3_col0\" class=\"data row3 col0\" >40 фильмов</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x299894df0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct some mistakes in movies names\n",
    "movie_labels.movie = movie_labels.movie.str.replace('.srt', '', regex=False)\n",
    "movie_labels.loc[movie_labels.movie == 'Up (2009)', 'movie'] = 'Up(2009)'\n",
    "movie_labels.loc[movie_labels.movie == 'The Grinch', 'movie'] = 'The.Grinch'\n",
    "\n",
    "# удалим из level лишние символы, разделители оставим пробел\n",
    "# из мультиуровней выберем наибольший\n",
    "movie_labels.level = movie_labels.level \\\n",
    "                                 .str.replace(',', '', regex=False) \\\n",
    "                                 .str.replace('+', '', regex=False) \\\n",
    "                                 .str.replace('/', ' ', regex=False) \\\n",
    "                                 .str.split().transform(lambda x: max(x))\n",
    "\n",
    "movie_labels.groupby('level').count().style.format({'movie':'{:.0f} фильмов'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919b2f0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не найден текст The Secret Life of Pets.en\n",
      "не найден текст Glass Onion\n",
      "не найден текст Matilda(2022)\n",
      "не найден текст Bullet train\n",
      "не найден текст Thor: love and thunder\n",
      "не найден текст Lightyear\n"
     ]
    }
   ],
   "source": [
    "# excel processing\n",
    "for row in movie_labels.itertuples():\n",
    "\n",
    "    n = movies.loc[movies.filename.str.contains(row.movie, regex=False)].shape[0]\n",
    "\n",
    "    if n == 0:\n",
    "        print('не найден текст', row.movie)\n",
    "\n",
    "    elif n == 1:\n",
    "        selected_movie_level = movies.loc[\n",
    "            movies.filename.str.contains(row.movie, regex=False), 'level'].values[0]\n",
    "\n",
    "        if selected_movie_level == 'Subtitles':         # replace Subtitles with excel level\n",
    "             movies.loc[\n",
    "                 movies.filename.str.contains(row.movie, regex=False), 'level'] = row.level\n",
    "\n",
    "        elif selected_movie_level != row.level:          # replace with max current level or excel\n",
    "             movies.loc[\n",
    "                 movies.filename.str.contains(row.movie, regex=False), 'level'\n",
    "             ] = max(selected_movie_level, row.level)\n",
    "\n",
    "    else:\n",
    "        print('не единственный текст', row.movie)\n",
    "\n",
    "\n",
    "movies = movies[movies.level!='Subtitles']\n",
    "\n",
    "# movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce311556",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9b929",
   "metadata": {},
   "source": [
    "Подготовим текст для машинного обучения:\n",
    "- оставляем только буквы, апострофы и пробелы \n",
    "- выполним лемматизацию – приведение слов к нормальной форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# movies content lemmatization\n",
    "movies['lemmas'] = movies.content.transform(lambda text: \n",
    "                                            ' '.join([w.lemma_.strip() for w \n",
    "                                                      in nlp(re.sub(r'[^a-z\\s\\']', '', text))\n",
    "                                                     ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9be3d5",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb205b6",
   "metadata": {},
   "source": [
    "Рассчитаем и добавим численные признаки.\n",
    "\n",
    "Количество слов (лемм):\n",
    "- общее количество\n",
    "- количество уникальных слов\n",
    "- доля уников от общего количества\n",
    "\n",
    "Сложность слов:\n",
    "- подсчитаем количество уникальных слов каждого уровня\n",
    "- доля слов каждой сложности относительно общего количества уников\n",
    "\n",
    "Целевой признак перекодируем в порядовые числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total lemmas count\n",
    "# and unique lemmas count\n",
    "# and unique ratio\n",
    "movies['lemmas_count']  = movies.lemmas.transform(lambda x: len(x.split()))\n",
    "movies['lemmas_unique'] = movies.lemmas.transform(lambda x: len(set(x.split())))\n",
    "movies['lemmas_unique_ratio'] = movies.lemmas_unique / movies.lemmas_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oxford words in movies lemmas\n",
    "def count_words_by_level(row):\n",
    "    row_lemmas = row.lemmas.split()\n",
    "    total_score = oxford_words.loc[oxford_words.word.isin(row_lemmas)] \\\n",
    "                              .groupby('level').word.count().to_dict()\n",
    "    row[total_score.keys()] = total_score\n",
    "    return row\n",
    "\n",
    "movies = movies.apply(count_words_by_level, axis=1)\n",
    "\n",
    "# ratio oxford words in content\n",
    "for l in ENGLISH_LEVELS:\n",
    "    movies[l+'ratio'] = movies[l]/movies.lemmas_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58899604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words by length\n",
    "# and ratio to total count\n",
    "for i in range(1,10):\n",
    "    equal_col = 'len_equal_'+str(i)\n",
    "    more_col  = 'more_than_'+str(i)\n",
    "    less_col  = 'less_than_'+str(i)\n",
    "    \n",
    "    movies[equal_col] = movies.lemmas.transform(lambda x: sum(len(word)==i for word in x.split()))\n",
    "    movies[more_col]  = movies.lemmas.transform(lambda x: sum(len(word)>i for word in x.split()))\n",
    "    movies[less_col]  = movies.lemmas.transform(lambda x: sum(len(word)<i for word in x.split()))\n",
    "    \n",
    "    movies[equal_col + 'ratio'] = movies[equal_col] / movies.lemmas_count\n",
    "    movies[more_col + 'ratio']  = movies[more_col] / movies.lemmas_count\n",
    "    movies[less_col + 'ratio']  = movies[less_col] / movies.lemmas_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c297e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode level to ordinal values\n",
    "# fill missing values\n",
    "# export data movies to csv\n",
    "movies['target'] = movies.level.replace({'A1':0, 'A2':1, 'B1':2, 'B2':3, 'C1':4, 'C2':5})\n",
    "movies = movies.fillna(0)\n",
    "movies.to_csv('movies_df.csv', index=False)\n",
    "\n",
    "try:\n",
    "    os.system('say \"data processing completed\"')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb55cf",
   "metadata": {},
   "source": [
    "# MODELS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf1f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84702de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'lemmas'\n",
    "\n",
    "lemmas_num = ['lemmas_count', 'lemmas_unique', 'lemmas_unique_ratio']\n",
    "\n",
    "levels_ratio = [l+'ratio' for l in ENGLISH_LEVELS]\n",
    "\n",
    "equal_ratio = [c+'ratio' for c in length_equal]\n",
    "more_ratio  = [c+'ratio' for c in length_more]\n",
    "less_ratio  = [c+'ratio' for c in length_less]\n",
    "\n",
    "num = [*lemmas_num,\n",
    "       *levels_ratio,\n",
    "       *equal_ratio,\n",
    "       *more_ratio,\n",
    "       *less_ratio,\n",
    "      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74532b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(movies, \n",
    "                                                    movies.target,\n",
    "                                                    stratify=movies.target,\n",
    "                                                    random_state=RANDOM_STATE,\n",
    "                                                    test_size=0.3\n",
    "                                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = CountVectorizer()\n",
    "scaler     = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', scaler, num),\n",
    "    ('txt', vectorizer,  text)\n",
    "], remainder='drop')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3402d",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517e5f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    'pre__txt__stop_words'  : [None, 'english', list(stop_words.STOP_WORDS)+['ll', 've']],\n",
    "    'pre__txt__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "    'pre__txt__max_features': [None, 1000],\n",
    "    'pre__txt__min_df'      : [1, 10, 100],\n",
    "    'reg__learning_rate'    : [0.05, 0.1, 0.2],\n",
    "    'reg__n_estimators'     : [50, 100, 200],\n",
    "    'reg__max_depth'        : [3, 5, 7],\n",
    "    'reg__min_child_weight' : [1, 3, 5],\n",
    "    'reg__gamma'            : [0, 0.1, 0.5, 1],\n",
    "    'reg__subsample'        : [0.6, 0.8, 1.0],\n",
    "    'reg__colsample_bytree' : [0.6, 0.8, 1.0],\n",
    "    'reg__reg_alpha'        : [0, 0.1, 1],\n",
    "    'reg__reg_lambda'       : [0, 0.1, 1],\n",
    "    'reg__gamma'            : [0, 1, 5]\n",
    "}\n",
    "\n",
    "regressor = xgboost.XGBRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', regressor)\n",
    "])\n",
    "\n",
    "rc = RandomizedSearchCV(pipeline,\n",
    "                        param_distributions=params,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        error_score='raise',\n",
    "                        random_state=RANDOM_STATE,\n",
    "                        n_jobs=-1\n",
    "                       )\n",
    "\n",
    "rc.fit(X_train,y_train)\n",
    "\n",
    "try:\n",
    "    os.system('say \"Model training completed\"')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "-rc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab048eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(rc, 'english_level_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db172dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.corrwith(movies.target)[:-1].round(2).sort_values(ascending=False).plot()\n",
    "plt.xticks(rotation=90,size='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931194de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_names = rc.best_estimator_[0].get_feature_names_out()\n",
    "feature_importances = rc.best_estimator_[-1].feature_importances_\n",
    "fi = pd.DataFrame({'feature':feature_names, 'weight': feature_importances})\n",
    "fi.feature = fi.feature.str[5:]\n",
    "\n",
    "fi.sort_values(by='weight', ascending=False) \\\n",
    "  .head(20).sort_values(by='weight',ascending=True) \\\n",
    "  .plot(kind='barh', y='weight', x='feature', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "oxford_words['length'] = oxford_words.word.str.len()\n",
    "oxford_words.groupby('level').length.median().plot(kind='bar')\n",
    "plt.title('Медианная длина слова в словаре Oxford')\n",
    "plt.ylabel('Количество символов')\n",
    "plt.xlabel('Уровень английского')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce73c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
