{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a16cd5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "except:\n",
    "    !pip install pypdf\n",
    "    from pypdf import PdfReader\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    !pip install spacy\n",
    "    !python -m spacy download en\n",
    "    import spacy\n",
    "\n",
    "try:\n",
    "    import chardet\n",
    "except:\n",
    "    !pip install chardet\n",
    "    import chardet\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, mean_squared_error, f1_score\n",
    "\n",
    "import xgboost\n",
    "import catboost\n",
    "\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8da1824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('ggplot')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "SCORES_PATH    = 'English_level/English_scores'\n",
    "SUBTITLES_PATH = 'English_level/English_scores/Subtitles_all'\n",
    "OXFORD_PATH    = 'English_level/Oxford_CEFR_level'\n",
    "\n",
    "ENGLISH_LEVELS = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "RANDOM_STATE = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f98292",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99487174",
   "metadata": {},
   "source": [
    "## The Oxford core by CEFR level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f00b3d3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American_Oxford_3000_by_CEFR_level.pdf\n",
      "The_Oxford_3000_by_CEFR_level.pdf\n",
      "The_Oxford_5000_by_CEFR_level.pdf\n",
      "American_Oxford_5000_by_CEFR_level.pdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_983ec\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_983ec_level0_col0\" class=\"col_heading level0 col0\" >word</th>\n",
       "      <th id=\"T_983ec_level0_col1\" class=\"col_heading level0 col1\" >source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >level</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_983ec_level0_row0\" class=\"row_heading level0 row0\" >A1</th>\n",
       "      <td id=\"T_983ec_row0_col0\" class=\"data row0 col0\" >735 слов</td>\n",
       "      <td id=\"T_983ec_row0_col1\" class=\"data row0 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983ec_level0_row1\" class=\"row_heading level0 row1\" >A2</th>\n",
       "      <td id=\"T_983ec_row1_col0\" class=\"data row1 col0\" >747 слов</td>\n",
       "      <td id=\"T_983ec_row1_col1\" class=\"data row1 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983ec_level0_row2\" class=\"row_heading level0 row2\" >B1</th>\n",
       "      <td id=\"T_983ec_row2_col0\" class=\"data row2 col0\" >758 слов</td>\n",
       "      <td id=\"T_983ec_row2_col1\" class=\"data row2 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983ec_level0_row3\" class=\"row_heading level0 row3\" >B2</th>\n",
       "      <td id=\"T_983ec_row3_col0\" class=\"data row3 col0\" >1,446 слов</td>\n",
       "      <td id=\"T_983ec_row3_col1\" class=\"data row3 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_983ec_level0_row4\" class=\"row_heading level0 row4\" >C1</th>\n",
       "      <td id=\"T_983ec_row4_col0\" class=\"data row4 col0\" >1,198 слов</td>\n",
       "      <td id=\"T_983ec_row4_col1\" class=\"data row4 col1\" >['American_Oxford_5000_by_CEFR_level.pdf']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2c7739f10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_pdf(content):\n",
    "    \n",
    "    current_level = ''\n",
    "    words_by_level = {key:[] for key in ENGLISH_LEVELS}\n",
    "    \n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "\n",
    "        if 'Oxford' in line or 'English' in line:\n",
    "            pass\n",
    "        \n",
    "        elif line in ENGLISH_LEVELS:\n",
    "            level = line\n",
    "            \n",
    "        elif ' ' in line:\n",
    "            line = re.sub(r'\\d|,', '', line.lower())   # remove digits and commas and transform to lowercase\n",
    "            words_by_level[level] += [line.split()[0]] #get the first occurency in line as word\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return words_by_level\n",
    "\n",
    "\n",
    "content = []\n",
    "oxford_words = pd.DataFrame(columns=['word', 'level', 'source', 'type'])\n",
    "\n",
    "# load all pdfs\n",
    "for dirname, _, filenames in os.walk(OXFORD_PATH):\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        reader = PdfReader(f'{OXFORD_PATH}/{filename}')\n",
    "        for page in reader.pages:\n",
    "            content += page.extract_text().splitlines()\n",
    "        processed = process_pdf(content)\n",
    "        for level in processed:\n",
    "            dict_type = 'us' if 'American' in filename else 'uk'\n",
    "            oxford_words = pd.concat([oxford_words,\n",
    "                                      pd.DataFrame({'word'   : processed[level],\n",
    "                                                    'level'  : level,\n",
    "                                                    'source' : filename,\n",
    "                                                    'type'   : dict_type\n",
    "                                                   })\n",
    "                                     ])\n",
    "\n",
    "# sort words and keep unique with higher levels\n",
    "oxford_words = oxford_words.sort_values(by=['word', 'level'], ascending=True)\n",
    "oxford_words = oxford_words.drop_duplicates(subset=['word'], keep='last')\n",
    "\n",
    "# после сортировки и удаления дубликатов остался только американский инглиш\n",
    "# похоже там все уровни слов считаются выше\n",
    "\n",
    "oxford_words.groupby('level') \\\n",
    "            .agg({'word':'count', 'source': 'unique'}) \\\n",
    "            .style.format({'word':'{:,.0f} слов'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f5ed1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>level</th>\n",
       "      <th>source</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>a</td>\n",
       "      <td>A1</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>abandon</td>\n",
       "      <td>B2</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>ability</td>\n",
       "      <td>A2</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>able</td>\n",
       "      <td>A2</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>abolish</td>\n",
       "      <td>C1</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>yours</td>\n",
       "      <td>A2</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>yourself</td>\n",
       "      <td>A1</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>youth</td>\n",
       "      <td>B1</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>zero</td>\n",
       "      <td>A2</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>zone</td>\n",
       "      <td>B2</td>\n",
       "      <td>American_Oxford_5000_by_CEFR_level.pdf</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4884 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word level                                  source type\n",
       "887          a    A1  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "716    abandon    B2  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "856    ability    A2  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "857       able    A2  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "1109   abolish    C1  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "...        ...   ...                                     ...  ...\n",
       "1714     yours    A2  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "1773  yourself    A1  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "1594     youth    B1  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "1715      zero    A2  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "1432      zone    B2  American_Oxford_5000_by_CEFR_level.pdf   us\n",
       "\n",
       "[4884 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxford_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b4e68fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oxford_dictionary.joblib']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(oxford_words,'oxford_dictionary.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d6b2f",
   "metadata": {},
   "source": [
    "## Subtitles .srt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9555659b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "посторонний файл .DS_Store\n",
      "посторонний файл .DS_Store\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>raw</th>\n",
       "      <th>content</th>\n",
       "      <th>level</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "      <th>...</th>\n",
       "      <th>A1us</th>\n",
       "      <th>A2us</th>\n",
       "      <th>B1us</th>\n",
       "      <th>B2us</th>\n",
       "      <th>C1us</th>\n",
       "      <th>A1ratio</th>\n",
       "      <th>A2ratio</th>\n",
       "      <th>B1ratio</th>\n",
       "      <th>B2ratio</th>\n",
       "      <th>C1ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crown, The S01E01 - Wolferton Splash.en</td>\n",
       "      <td>1\\r\\n00:00:59,400 --&gt; 00:01:03,960\\r\\nIn seeki...</td>\n",
       "      <td>in seeking his british nationalization his roy...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suits.Episode 1- Denial</td>\n",
       "      <td>1\\r\\n00:00:07,298 --&gt; 00:00:09,332\\r\\nYou're t...</td>\n",
       "      <td>youre the most amazing woman i have ever met a...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...</td>\n",
       "      <td>1\\r\\n00:00:00,877 --&gt; 00:00:02,337\\r\\n(HARVEY ...</td>\n",
       "      <td>ive been after sutter for three years now this...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suits.S02E08.HDTV.x264-EVOLVE</td>\n",
       "      <td>1\\r\\n00:00:05,640 --&gt; 00:00:07,641\\r\\n[Car hor...</td>\n",
       "      <td>youre late nope seconds early good learning to...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE</td>\n",
       "      <td>1\\r\\n00:00:10,468 --&gt; 00:00:13,178\\r\\nAre you ...</td>\n",
       "      <td>are you sure i cant convince you to stay no yo...</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0            Crown, The S01E01 - Wolferton Splash.en   \n",
       "1                            Suits.Episode 1- Denial   \n",
       "2  Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...   \n",
       "3                      Suits.S02E08.HDTV.x264-EVOLVE   \n",
       "4  Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE   \n",
       "\n",
       "                                                 raw  \\\n",
       "0  1\\r\\n00:00:59,400 --> 00:01:03,960\\r\\nIn seeki...   \n",
       "1  1\\r\\n00:00:07,298 --> 00:00:09,332\\r\\nYou're t...   \n",
       "2  1\\r\\n00:00:00,877 --> 00:00:02,337\\r\\n(HARVEY ...   \n",
       "3  1\\r\\n00:00:05,640 --> 00:00:07,641\\r\\n[Car hor...   \n",
       "4  1\\r\\n00:00:10,468 --> 00:00:13,178\\r\\nAre you ...   \n",
       "\n",
       "                                             content level  lemmas  A1  A2  \\\n",
       "0  in seeking his british nationalization his roy...    B2     NaN NaN NaN   \n",
       "1  youre the most amazing woman i have ever met a...    B2     NaN NaN NaN   \n",
       "2  ive been after sutter for three years now this...    B2     NaN NaN NaN   \n",
       "3  youre late nope seconds early good learning to...    B2     NaN NaN NaN   \n",
       "4  are you sure i cant convince you to stay no yo...    B2     NaN NaN NaN   \n",
       "\n",
       "   B1  B2  C1  ...  A1us  A2us  B1us  B2us  C1us  A1ratio  A2ratio  B1ratio  \\\n",
       "0 NaN NaN NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN      NaN      NaN   \n",
       "1 NaN NaN NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN      NaN      NaN   \n",
       "2 NaN NaN NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN      NaN      NaN   \n",
       "3 NaN NaN NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN      NaN      NaN   \n",
       "4 NaN NaN NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN      NaN      NaN   \n",
       "\n",
       "   B2ratio  C1ratio  \n",
       "0      NaN      NaN  \n",
       "1      NaN      NaN  \n",
       "2      NaN      NaN  \n",
       "3      NaN      NaN  \n",
       "4      NaN      NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse and clean subtitles   \n",
    "def clean_text(content):\n",
    "    text = []\n",
    "    for line in content.splitlines():                        \n",
    "        if re.search(r'[A-Za-z]',line):                      # get non empty lines with words\n",
    "            line = line.lower()                              # transform to lowercase\n",
    "            line = re.sub(r'\\<[^<]+?\\>', ' ', line)          # remove html tags\n",
    "            line = re.sub(r'(\\(.+\\))|(\\[.+\\])', ' ', line)   # remove actions in parenthesis\n",
    "            line = re.sub(r'([\\w#\\s]+\\:)', ' ', line)        # remove speaker in dialogs\n",
    "            line = re.sub(r'(?<=\\w)\\'(?=\\w)', '', line)      # glue apostroph inside words\n",
    "            line = re.sub(r'[^a-z]', ' ', line)              # remove non-word symbols\n",
    "#             line = re.sub(r'\\b\\w{1,2}\\b', '', line)          # remove single and two symbol sequences\n",
    "            line = re.sub(r'\\s\\s+', ' ', line).strip()       # remove extra spaces longer than single\n",
    "            if len(line)>0:\n",
    "                text.append(line)\n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "# process file\n",
    "def process_srt(dirname, filename):\n",
    "    global count\n",
    "    if not filename.endswith('.srt'):                        # skip non srt files\n",
    "        print('посторонний файл', filename)\n",
    "        return '', False\n",
    "    fullpath = os.path.join(dirname,filename)\n",
    "    try:\n",
    "        file = open(fullpath, 'rb')\n",
    "        content = file.read()\n",
    "        file.close()\n",
    "        encoding = chardet.detect(content)['encoding']      # detect encoding and convert\n",
    "#         print(encoding)\n",
    "        content = content.decode(encoding)\n",
    "    except:\n",
    "        print('не прочиталось', filename)\n",
    "        return '', False, \n",
    "    return content, clean_text(content)                     # clean text and return\n",
    "\n",
    "movies = pd.DataFrame(columns=['filename', 'raw', 'content', 'level', 'lemmas'] + ENGLISH_LEVELS + \\\n",
    "                              [l+'uk' for l in ENGLISH_LEVELS] + \\\n",
    "                              [l+'us' for l in ENGLISH_LEVELS] + \\\n",
    "                              [l+'ratio' for l in ENGLISH_LEVELS]\n",
    "                     )\n",
    "\n",
    "# recursive walk through dirs\n",
    "for dirname, _, filenames in os.walk(SUBTITLES_PATH):\n",
    "    for filename in filenames:\n",
    "        level  = dirname.split('/')[-1]                       # get level name from dir\n",
    "        raw, subs = process_srt(dirname, filename)            # process file\n",
    "        if subs:                                              # add movie to dataframe\n",
    "            movies.loc[len(movies)] = \\\n",
    "                {'filename': filename.replace('.srt', ''),\n",
    "                 'raw'     : raw,\n",
    "                 'content' : subs,\n",
    "                 'level'   : level\n",
    "                }\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e818948f",
   "metadata": {},
   "source": [
    "## Excel labels processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e329ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 241 entries, 0 to 240\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Movie   241 non-null    object\n",
      " 1   Level   241 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ KB\n",
      "\n",
      "Дубликаты полные 2\n",
      "Дубликаты в названиях фильмов 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Bullet train</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Lightyear</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>The Grinch</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                movie   level\n",
       "id                                           \n",
       "0           10_Cloverfield_lane(2016)      B1\n",
       "1    10_things_I_hate_about_you(1999)      B1\n",
       "2                A_knights_tale(2001)      B2\n",
       "3                A_star_is_born(2018)      B2\n",
       "4                       Aladdin(1992)  A2/A2+\n",
       "..                                ...     ...\n",
       "236                     Matilda(2022)      C1\n",
       "237                      Bullet train      B1\n",
       "238            Thor: love and thunder      B2\n",
       "239                         Lightyear      B2\n",
       "240                        The Grinch      B1\n",
       "\n",
       "[239 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_labels = pd.read_excel(f'{SCORES_PATH}/movies_labels.xlsx', index_col='id')\n",
    "movie_labels.info()\n",
    "movie_labels.columns = ['movie', 'level']\n",
    "print('\\nДубликаты полные', movie_labels.duplicated().sum())\n",
    "print('Дубликаты в названиях фильмов', movie_labels.movie.duplicated().sum())\n",
    "movie_labels = movie_labels.drop_duplicates()\n",
    "movie_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0885dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_57234\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_57234_level0_col0\" class=\"col_heading level0 col0\" >movie</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >level</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_57234_level0_row0\" class=\"row_heading level0 row0\" >A2</th>\n",
       "      <td id=\"T_57234_row0_col0\" class=\"data row0 col0\" >37 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57234_level0_row1\" class=\"row_heading level0 row1\" >B1</th>\n",
       "      <td id=\"T_57234_row1_col0\" class=\"data row1 col0\" >61 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57234_level0_row2\" class=\"row_heading level0 row2\" >B2</th>\n",
       "      <td id=\"T_57234_row2_col0\" class=\"data row2 col0\" >101 фильмов</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57234_level0_row3\" class=\"row_heading level0 row3\" >C1</th>\n",
       "      <td id=\"T_57234_row3_col0\" class=\"data row3 col0\" >40 фильмов</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x293a17df0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct some mistakes in movies names\n",
    "movie_labels.movie = movie_labels.movie.str.replace('.srt', '', regex=False)\n",
    "movie_labels.loc[movie_labels.movie == 'Up (2009)', 'movie'] = 'Up(2009)'\n",
    "movie_labels.loc[movie_labels.movie == 'The Grinch', 'movie'] = 'The.Grinch'\n",
    "\n",
    "# удалим из level лишние символы, разделители оставим пробел\n",
    "# из мультиуровней выберем наименьший\n",
    "movie_labels.level = movie_labels.level \\\n",
    "                                 .str.replace(',', '', regex=False) \\\n",
    "                                 .str.replace('+', '', regex=False) \\\n",
    "                                 .str.replace('/', ' ', regex=False) \\\n",
    "                                 .str.split().transform(lambda x: min(x))\n",
    "\n",
    "movie_labels.groupby('level').count().style.format({'movie':'{:.0f} фильмов'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b400589b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>filename</th>\n",
       "      <th>content</th>\n",
       "      <th>level</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "      <th>...</th>\n",
       "      <th>coverage_more_than_B1ratio</th>\n",
       "      <th>coverage_less_equal_B2</th>\n",
       "      <th>coverage_less_equal_B2ratio</th>\n",
       "      <th>coverage_more_than_B2</th>\n",
       "      <th>coverage_more_than_B2ratio</th>\n",
       "      <th>coverage_less_equal_C1</th>\n",
       "      <th>coverage_less_equal_C1ratio</th>\n",
       "      <th>coverage_more_than_C1</th>\n",
       "      <th>coverage_more_than_C1ratio</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Braveheart(1995)</td>\n",
       "      <td>i shall tell you of william wallace historians...</td>\n",
       "      <td>B2</td>\n",
       "      <td>I shall tell you of william wallace historians...</td>\n",
       "      <td>278</td>\n",
       "      <td>176</td>\n",
       "      <td>152</td>\n",
       "      <td>150</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081694</td>\n",
       "      <td>756</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>66</td>\n",
       "      <td>0.055092</td>\n",
       "      <td>822</td>\n",
       "      <td>0.168305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The_Devil_Wears_Prad</td>\n",
       "      <td>good luck hi i have an appointment with emily ...</td>\n",
       "      <td>B2</td>\n",
       "      <td>good luck hi I have an appointment with emily ...</td>\n",
       "      <td>414</td>\n",
       "      <td>219</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092284</td>\n",
       "      <td>981</td>\n",
       "      <td>0.266142</td>\n",
       "      <td>68</td>\n",
       "      <td>0.056761</td>\n",
       "      <td>1049</td>\n",
       "      <td>0.214783</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     movie              filename  \\\n",
       "185    0.0      Braveheart(1995)   \n",
       "191    0.0  The_Devil_Wears_Prad   \n",
       "\n",
       "                                               content level  \\\n",
       "185  i shall tell you of william wallace historians...    B2   \n",
       "191  good luck hi i have an appointment with emily ...    B2   \n",
       "\n",
       "                                                lemmas   A1   A2   B1   B2  \\\n",
       "185  I shall tell you of william wallace historians...  278  176  152  150   \n",
       "191  good luck hi I have an appointment with emily ...  414  219  172  176   \n",
       "\n",
       "       C1  ...  coverage_more_than_B1ratio  coverage_less_equal_B2  \\\n",
       "185  66.0  ...                    0.081694                     756   \n",
       "191  68.0  ...                    0.092284                     981   \n",
       "\n",
       "     coverage_less_equal_B2ratio  coverage_more_than_B2  \\\n",
       "185                     0.205100                     66   \n",
       "191                     0.266142                     68   \n",
       "\n",
       "     coverage_more_than_B2ratio  coverage_less_equal_C1  \\\n",
       "185                    0.055092                     822   \n",
       "191                    0.056761                    1049   \n",
       "\n",
       "     coverage_less_equal_C1ratio  coverage_more_than_C1  \\\n",
       "185                     0.168305                      0   \n",
       "191                     0.214783                      0   \n",
       "\n",
       "     coverage_more_than_C1ratio  target  \n",
       "185                           0       4  \n",
       "191                           0       4  \n",
       "\n",
       "[2 rows x 103 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[movies.filename.str.contains('ear')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919b2f0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не найден текст The Secret Life of Pets.en\n",
      "не найден текст Glass Onion\n",
      "не найден текст Matilda(2022)\n",
      "не найден текст Bullet train\n",
      "не найден текст Thor: love and thunder\n",
      "не найден текст Lightyear\n"
     ]
    }
   ],
   "source": [
    "# excel processing\n",
    "for row in movie_labels.itertuples():\n",
    "\n",
    "    n = movies.loc[movies.filename.str.contains(row.movie, regex=False)].shape[0]\n",
    "\n",
    "    if n == 0:\n",
    "        print('не найден текст', row.movie)\n",
    "\n",
    "    elif n == 1:\n",
    "        selected_movie_level = movies.loc[\n",
    "            movies.filename.str.contains(row.movie, regex=False), 'level'].values[0]\n",
    "\n",
    "        if selected_movie_level == 'Subtitles':         # replace Subtitles with excel level\n",
    "             movies.loc[\n",
    "                 movies.filename.str.contains(row.movie, regex=False), 'level'] = row.level\n",
    "\n",
    "        elif selected_movie_level != row.level:          # replace with min current level or excel\n",
    "             movies.loc[\n",
    "                 movies.filename.str.contains(row.movie, regex=False), 'level'\n",
    "             ] = min(selected_movie_level, row.level)\n",
    "\n",
    "    else:\n",
    "        print('не единственный текст', row.movie)\n",
    "            \n",
    "# movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75304f88",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6794a198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 53s, sys: 15.8 s, total: 9min 9s\n",
      "Wall time: 9min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# movies content lemmatization\n",
    "movies['lemmas'] = movies.content.transform(lambda text: ' '.join([w.lemma_ for w in nlp(text)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab508e11",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d93ebeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oxford words in movies lemmas\n",
    "def count_words_by_level(row):\n",
    "\n",
    "    row_lemmas = row.lemmas.split()\n",
    "    \n",
    "    total_score = oxford_words.loc[oxford_words.word.isin(row_lemmas)] \\\n",
    "                              .groupby('level').word.count().to_dict()\n",
    "    \n",
    "#     british_score = oxford_words.loc[\n",
    "#         (oxford_words.type == 'uk') & \\\n",
    "#         (oxford_words.word.isin(row_lemmas))\n",
    "#     ].groupby('level').word.count().to_dict()\n",
    "\n",
    "#     american_score = oxford_words.loc[\n",
    "#         (oxford_words.type == 'us') & \\\n",
    "#         (oxford_words.word.isin(row_lemmas))\n",
    "#     ].groupby('level').word.count().to_dict()\n",
    "\n",
    "#     total_score.update({k+'en':v for k,v in british_score.items()})\n",
    "#     total_score.update({k+'us':v for k,v in american_score.items()})\n",
    "    \n",
    "    row[total_score.keys()] = total_score\n",
    "\n",
    "    return row\n",
    "\n",
    "movies = movies.apply(count_words_by_level, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469b148",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# coverage of oxford dict and ratio of coverage        \n",
    "def coverage_oxford(row):\n",
    "    \n",
    "    global l, oxford_equal, oxford_less, oxford_more\n",
    "    row_lemmas = set(row.lemmas.split())\n",
    "    \n",
    "    cover_col = 'coverage_' + l\n",
    "    ratio_col = 'coverage_' + l + 'ratio'\n",
    "    \n",
    "    cover_less = 'coverage_less_equal_' + l\n",
    "    ratio_less = 'coverage_less_equal_' + l + 'ratio'\n",
    "\n",
    "    cover_more = 'coverage_more_than_' + l\n",
    "    ratio_more = 'coverage_more_than_' + l + 'ratio'\n",
    "    \n",
    "    # покрытие словаря в фильме (по уровням)\n",
    "    row[cover_col] = sum(lemma in oxford_equal for lemma in row_lemmas)\n",
    "    row[ratio_col] = row[cover_col] / len(oxford_equal)\n",
    "\n",
    "    # покрытие словаря суммарно по уровням до заданного включительно\n",
    "    row[cover_less] = sum(lemma in oxford_less for lemma in row_lemmas)\n",
    "    row[ratio_less] = row[cover_less] / len(oxford_less)\n",
    "\n",
    "    # покрытие словаря выше заданного уровня\n",
    "    len_oxford_more = len(oxford_more)\n",
    "    if len_oxford_more>0:\n",
    "        row[cover_more] = sum(lemma in oxford_more for lemma in row_lemmas)\n",
    "        row[ratio_more] = row[cover_more] / len(oxford_more)\n",
    "    else:\n",
    "        row[cover_more], row[ratio_more] = 0, 0\n",
    "\n",
    "    return row\n",
    "\n",
    "for l in ENGLISH_LEVELS:\n",
    "    oxford_equal = oxford_words[oxford_words.level==l].word.values\n",
    "    oxford_less  = oxford_words[oxford_words.level<=l].word.values\n",
    "    oxford_more  = oxford_words[oxford_words.level>l].word.values\n",
    "    movies = movies.apply(coverage_oxford, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total lemmas count\n",
    "# and unique lemmas count\n",
    "# and unique ratio\n",
    "movies['lemmas_count']  = movies.lemmas.transform(lambda x: len(x.split()))\n",
    "movies['lemmas_unique'] = movies.lemmas.transform(lambda x: len(set(x.split())))\n",
    "movies['lemmas_unique_ratio'] = movies.lemmas_unique / movies.lemmas_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b714a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio oxford words in content\n",
    "for l in ENGLISH_LEVELS:\n",
    "    movies[l+'ratio'] = movies[l]/movies.lemmas_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c90df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words by length\n",
    "# and ratio to total count\n",
    "for i in range(1,10):\n",
    "    equal_col = 'equal_'+str(i)\n",
    "    more_col  = 'more_than_'+str(i)\n",
    "    less_col  = 'less_than_'+str(i)\n",
    "    \n",
    "    movies[equal_col] = movies.lemmas.transform(lambda x: sum(len(word)==i for word in x.split()))\n",
    "    movies[more_col]  = movies.lemmas.transform(lambda x: sum(len(word)>i for word in x.split()))\n",
    "    movies[less_col]  = movies.lemmas.transform(lambda x: sum(len(word)<i for word in x.split()))\n",
    "    \n",
    "    movies[equal_col + 'ratio'] = movies[equal_col] / movies.lemmas_count\n",
    "    movies[more_col + 'ratio']  = movies[more_col] / movies.lemmas_count\n",
    "    movies[less_col + 'ratio']  = movies[less_col] / movies.lemmas_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "5c297e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data movies to csv\n",
    "movies['target'] = movies.level.replace({'A1':1, 'A2':2, 'B1':3, 'B2':4, 'C1':5, 'C2':6})\n",
    "movies = movies.fillna(0)\n",
    "movies = movies[movies.level!='Subtitles']\n",
    "movies.to_csv('movies_df.csv', index=False)\n",
    "\n",
    "try:\n",
    "    os.system('say \"data processing completed\"')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb55cf",
   "metadata": {},
   "source": [
    "# MODELS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf1f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84702de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'lemmas'\n",
    "\n",
    "lemmas_num = ['lemmas_count', 'lemmas_unique', 'lemmas_unique_ratio']\n",
    "\n",
    "levels_ratio = [l+'ratio' for l in ENGLISH_LEVELS]\n",
    "# levels_us    = [l+'us' for l in ENGLISH_LEVELS]\n",
    "# levels_uk    = [l+'uk' for l in ENGLISH_LEVELS]\n",
    "\n",
    "length_equal = ['equal_'+str(i) for i in range(1,10)]\n",
    "length_more  = ['more_than_'+str(i) for i in range(1,10)]\n",
    "length_less  = ['less_than_'+str(i) for i in range(1,10)]\n",
    "\n",
    "equal_ratio = [c+'ratio' for c in length_equal]\n",
    "more_ratio  = [c+'ratio' for c in length_more]\n",
    "less_ratio  = [c+'ratio' for c in length_less]\n",
    "\n",
    "oxford_cover = ['coverage_' + l for l in ENGLISH_LEVELS]\n",
    "oxford_ratio = ['coverage_' + l + 'ratio' for l in ENGLISH_LEVELS]\n",
    "    \n",
    "oxford_cover_less= ['coverage_less_equal_' + l for l in ENGLISH_LEVELS]\n",
    "oxford_ratio_less= ['coverage_less_equal_' + l + 'ratio' for l in ENGLISH_LEVELS]\n",
    "\n",
    "oxford_cover_more= ['coverage_more_than_' + l for l in ENGLISH_LEVELS]\n",
    "oxford_ratio_more= ['coverage_more_than_' + l + 'ratio' for l in ENGLISH_LEVELS]\n",
    "\n",
    "\n",
    "# num_us = lemmas_num + levels_us\n",
    "# num_en = lemmas_num + levels_uk\n",
    "\n",
    "num = [*lemmas_num,\n",
    "       *levels_ratio,\n",
    "#        *length_equal,\n",
    "#        *length_more,\n",
    "#        *length_less,\n",
    "       *equal_ratio,\n",
    "       *more_ratio,\n",
    "       *less_ratio,\n",
    "#        *oxford_cover,\n",
    "       *oxford_ratio,\n",
    "#        *oxford_cover_less,\n",
    "       *oxford_ratio_less,\n",
    "#        *oxford_cover_more,\n",
    "       *oxford_ratio_more\n",
    "      ]\n",
    "\n",
    "target = 'level'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74532b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(movies, \n",
    "                                                    movies.target,\n",
    "#                                                     encoder.fit_transform(movies[[target]]), \n",
    "                                                    stratify=movies[[target]],\n",
    "                                                    random_state=RANDOM_STATE,\n",
    "                                                    test_size=0.3\n",
    "                                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ecd2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = CountVectorizer()\n",
    "scaler     = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', scaler, more_ratio),\n",
    "    ('txt', vectorizer,  text)\n",
    "], remainder='drop')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419225a6",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9517e5f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 s, sys: 1.07 s, total: 4.12 s\n",
      "Wall time: 25.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40931243274889384"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "#     'pre__txt__stop_words'  : [None, 'english'],\n",
    "#     'pre__txt__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "#     'pre__txt__max_features': [None, 1000],\n",
    "#     'pre__txt__min_df'      : [1, 10, 100],\n",
    "    'reg__learning_rate'    : [0.05, 0.1, 0.2],\n",
    "    'reg__n_estimators'     : [50, 100, 200],\n",
    "    'reg__max_depth'        : [3, 5, 7],\n",
    "    'reg__min_child_weight' : [1, 3, 5],\n",
    "    'reg__gamma'            : [0, 0.1, 0.5, 1],\n",
    "    'reg__subsample'        : [0.6, 0.8, 1.0],\n",
    "    'reg__colsample_bytree' : [0.6, 0.8, 1.0],\n",
    "    'reg__reg_alpha'        : [0, 0.1, 1],\n",
    "    'reg__reg_lambda'       : [0, 0.1, 1],\n",
    "    'reg__gamma'            : [0, 1, 5]\n",
    "}\n",
    "\n",
    "regressor = xgboost.XGBRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('pre', preprocessor),\n",
    "    ('reg', regressor)\n",
    "])\n",
    "\n",
    "rc = RandomizedSearchCV(pipeline,\n",
    "                        param_distributions=params,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        error_score='raise',\n",
    "                        random_state=RANDOM_STATE,\n",
    "                        n_jobs=-1\n",
    "                       )\n",
    "\n",
    "rc.fit(X_train,y_train)\n",
    "\n",
    "try:\n",
    "    os.system('say \"Model training completed\"')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "-rc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec0c9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                   estimator=Pipeline(steps=[(&#x27;pre&#x27;,\n",
       "                                              ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                                               StandardScaler(),\n",
       "                                                                               [&#x27;more_than_1ratio&#x27;,\n",
       "                                                                                &#x27;more_than_2ratio&#x27;,\n",
       "                                                                                &#x27;more_than_3ratio&#x27;,\n",
       "                                                                                &#x27;more_than_4ratio&#x27;,\n",
       "                                                                                &#x27;more_than_5ratio&#x27;,\n",
       "                                                                                &#x27;more_than_6ratio&#x27;,\n",
       "                                                                                &#x27;more_than_7ratio&#x27;,\n",
       "                                                                                &#x27;more_than_8ratio&#x27;,\n",
       "                                                                                &#x27;more_than_9ratio&#x27;]),\n",
       "                                                                              (&#x27;txt&#x27;,\n",
       "                                                                               CountVectorizer(),\n",
       "                                                                               &#x27;lemmas&#x27;)])),\n",
       "                                             (&#x27;reg&#x27;,\n",
       "                                              XGB...\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;reg__colsample_bytree&#x27;: [0.6, 0.8,\n",
       "                                                                  1.0],\n",
       "                                        &#x27;reg__gamma&#x27;: [0, 1, 5],\n",
       "                                        &#x27;reg__learning_rate&#x27;: [0.05, 0.1, 0.2],\n",
       "                                        &#x27;reg__max_depth&#x27;: [3, 5, 7],\n",
       "                                        &#x27;reg__min_child_weight&#x27;: [1, 3, 5],\n",
       "                                        &#x27;reg__n_estimators&#x27;: [50, 100, 200],\n",
       "                                        &#x27;reg__reg_alpha&#x27;: [0, 0.1, 1],\n",
       "                                        &#x27;reg__reg_lambda&#x27;: [0, 0.1, 1],\n",
       "                                        &#x27;reg__subsample&#x27;: [0.6, 0.8, 1.0]},\n",
       "                   random_state=7, scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                   estimator=Pipeline(steps=[(&#x27;pre&#x27;,\n",
       "                                              ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                                               StandardScaler(),\n",
       "                                                                               [&#x27;more_than_1ratio&#x27;,\n",
       "                                                                                &#x27;more_than_2ratio&#x27;,\n",
       "                                                                                &#x27;more_than_3ratio&#x27;,\n",
       "                                                                                &#x27;more_than_4ratio&#x27;,\n",
       "                                                                                &#x27;more_than_5ratio&#x27;,\n",
       "                                                                                &#x27;more_than_6ratio&#x27;,\n",
       "                                                                                &#x27;more_than_7ratio&#x27;,\n",
       "                                                                                &#x27;more_than_8ratio&#x27;,\n",
       "                                                                                &#x27;more_than_9ratio&#x27;]),\n",
       "                                                                              (&#x27;txt&#x27;,\n",
       "                                                                               CountVectorizer(),\n",
       "                                                                               &#x27;lemmas&#x27;)])),\n",
       "                                             (&#x27;reg&#x27;,\n",
       "                                              XGB...\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;reg__colsample_bytree&#x27;: [0.6, 0.8,\n",
       "                                                                  1.0],\n",
       "                                        &#x27;reg__gamma&#x27;: [0, 1, 5],\n",
       "                                        &#x27;reg__learning_rate&#x27;: [0.05, 0.1, 0.2],\n",
       "                                        &#x27;reg__max_depth&#x27;: [3, 5, 7],\n",
       "                                        &#x27;reg__min_child_weight&#x27;: [1, 3, 5],\n",
       "                                        &#x27;reg__n_estimators&#x27;: [50, 100, 200],\n",
       "                                        &#x27;reg__reg_alpha&#x27;: [0, 0.1, 1],\n",
       "                                        &#x27;reg__reg_lambda&#x27;: [0, 0.1, 1],\n",
       "                                        &#x27;reg__subsample&#x27;: [0.6, 0.8, 1.0]},\n",
       "                   random_state=7, scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pre&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                                  [&#x27;more_than_1ratio&#x27;,\n",
       "                                                   &#x27;more_than_2ratio&#x27;,\n",
       "                                                   &#x27;more_than_3ratio&#x27;,\n",
       "                                                   &#x27;more_than_4ratio&#x27;,\n",
       "                                                   &#x27;more_than_5ratio&#x27;,\n",
       "                                                   &#x27;more_than_6ratio&#x27;,\n",
       "                                                   &#x27;more_than_7ratio&#x27;,\n",
       "                                                   &#x27;more_than_8ratio&#x27;,\n",
       "                                                   &#x27;more_than_9ratio&#x27;]),\n",
       "                                                 (&#x27;txt&#x27;, CountVectorizer(),\n",
       "                                                  &#x27;lemmas&#x27;)])),\n",
       "                (&#x27;reg&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=...\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=None,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=None, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, n_estimators=100,\n",
       "                              n_jobs=None, num_parallel_tree=None,\n",
       "                              predictor=None, random_state=7, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pre: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
       "                                 [&#x27;more_than_1ratio&#x27;, &#x27;more_than_2ratio&#x27;,\n",
       "                                  &#x27;more_than_3ratio&#x27;, &#x27;more_than_4ratio&#x27;,\n",
       "                                  &#x27;more_than_5ratio&#x27;, &#x27;more_than_6ratio&#x27;,\n",
       "                                  &#x27;more_than_7ratio&#x27;, &#x27;more_than_8ratio&#x27;,\n",
       "                                  &#x27;more_than_9ratio&#x27;]),\n",
       "                                (&#x27;txt&#x27;, CountVectorizer(), &#x27;lemmas&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;more_than_1ratio&#x27;, &#x27;more_than_2ratio&#x27;, &#x27;more_than_3ratio&#x27;, &#x27;more_than_4ratio&#x27;, &#x27;more_than_5ratio&#x27;, &#x27;more_than_6ratio&#x27;, &#x27;more_than_7ratio&#x27;, &#x27;more_than_8ratio&#x27;, &#x27;more_than_9ratio&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">txt</label><div class=\"sk-toggleable__content\"><pre>lemmas</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=7, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(error_score='raise',\n",
       "                   estimator=Pipeline(steps=[('pre',\n",
       "                                              ColumnTransformer(transformers=[('num',\n",
       "                                                                               StandardScaler(),\n",
       "                                                                               ['more_than_1ratio',\n",
       "                                                                                'more_than_2ratio',\n",
       "                                                                                'more_than_3ratio',\n",
       "                                                                                'more_than_4ratio',\n",
       "                                                                                'more_than_5ratio',\n",
       "                                                                                'more_than_6ratio',\n",
       "                                                                                'more_than_7ratio',\n",
       "                                                                                'more_than_8ratio',\n",
       "                                                                                'more_than_9ratio']),\n",
       "                                                                              ('txt',\n",
       "                                                                               CountVectorizer(),\n",
       "                                                                               'lemmas')])),\n",
       "                                             ('reg',\n",
       "                                              XGB...\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'reg__colsample_bytree': [0.6, 0.8,\n",
       "                                                                  1.0],\n",
       "                                        'reg__gamma': [0, 1, 5],\n",
       "                                        'reg__learning_rate': [0.05, 0.1, 0.2],\n",
       "                                        'reg__max_depth': [3, 5, 7],\n",
       "                                        'reg__min_child_weight': [1, 3, 5],\n",
       "                                        'reg__n_estimators': [50, 100, 200],\n",
       "                                        'reg__reg_alpha': [0, 0.1, 1],\n",
       "                                        'reg__reg_lambda': [0, 0.1, 1],\n",
       "                                        'reg__subsample': [0.6, 0.8, 1.0]},\n",
       "                   random_state=7, scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd0b643f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_english_level.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(rc, 'model_english_level.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "931194de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>firm</td>\n",
       "      <td>0.054635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9137</th>\n",
       "      <td>harvey</td>\n",
       "      <td>0.041164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18220</th>\n",
       "      <td>shh</td>\n",
       "      <td>0.032929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>come</td>\n",
       "      <td>0.027545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7639</th>\n",
       "      <td>fly</td>\n",
       "      <td>0.026115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>more_than_6ratio</td>\n",
       "      <td>0.024185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22339</th>\n",
       "      <td>which</td>\n",
       "      <td>0.024103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9567</th>\n",
       "      <td>honest</td>\n",
       "      <td>0.023404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>more_than_2ratio</td>\n",
       "      <td>0.020367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6401</th>\n",
       "      <td>eld</td>\n",
       "      <td>0.016603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22647</th>\n",
       "      <td>would</td>\n",
       "      <td>0.015323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18480</th>\n",
       "      <td>sing</td>\n",
       "      <td>0.015092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13930</th>\n",
       "      <td>no</td>\n",
       "      <td>0.014669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5979</th>\n",
       "      <td>downton</td>\n",
       "      <td>0.014647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>america</td>\n",
       "      <td>0.014206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20014</th>\n",
       "      <td>talk</td>\n",
       "      <td>0.014157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18487</th>\n",
       "      <td>singing</td>\n",
       "      <td>0.014139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9536</th>\n",
       "      <td>home</td>\n",
       "      <td>0.014074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6253</th>\n",
       "      <td>east</td>\n",
       "      <td>0.013856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20840</th>\n",
       "      <td>travel</td>\n",
       "      <td>0.013303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20377</th>\n",
       "      <td>thief</td>\n",
       "      <td>0.012161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797</th>\n",
       "      <td>job</td>\n",
       "      <td>0.012094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18293</th>\n",
       "      <td>shoot</td>\n",
       "      <td>0.012007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>date</td>\n",
       "      <td>0.011787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6404</th>\n",
       "      <td>elderman</td>\n",
       "      <td>0.011414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19049</th>\n",
       "      <td>spend</td>\n",
       "      <td>0.011264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2792</th>\n",
       "      <td>bye</td>\n",
       "      <td>0.011264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>destiny</td>\n",
       "      <td>0.011252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22396</th>\n",
       "      <td>why</td>\n",
       "      <td>0.010121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>doc</td>\n",
       "      <td>0.009638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7226</th>\n",
       "      <td>favor</td>\n",
       "      <td>0.009354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22188</th>\n",
       "      <td>weak</td>\n",
       "      <td>0.008964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22613</th>\n",
       "      <td>work</td>\n",
       "      <td>0.008772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19870</th>\n",
       "      <td>swim</td>\n",
       "      <td>0.008596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>charmaine</td>\n",
       "      <td>0.008465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20527</th>\n",
       "      <td>till</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6482</th>\n",
       "      <td>em</td>\n",
       "      <td>0.007375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5127</th>\n",
       "      <td>deep</td>\n",
       "      <td>0.007319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9125</th>\n",
       "      <td>harry</td>\n",
       "      <td>0.006964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11132</th>\n",
       "      <td>kill</td>\n",
       "      <td>0.006929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492</th>\n",
       "      <td>chop</td>\n",
       "      <td>0.006920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>bed</td>\n",
       "      <td>0.006655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22255</th>\n",
       "      <td>well</td>\n",
       "      <td>0.006442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18879</th>\n",
       "      <td>somebody</td>\n",
       "      <td>0.006414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21177</th>\n",
       "      <td>um</td>\n",
       "      <td>0.006318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13022</th>\n",
       "      <td>mine</td>\n",
       "      <td>0.006198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111</th>\n",
       "      <td>kid</td>\n",
       "      <td>0.006177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9194</th>\n",
       "      <td>head</td>\n",
       "      <td>0.005670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18673</th>\n",
       "      <td>slow</td>\n",
       "      <td>0.005622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144</th>\n",
       "      <td>cb</td>\n",
       "      <td>0.005582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature    weight\n",
       "7463               firm  0.054635\n",
       "9137             harvey  0.041164\n",
       "18220               shh  0.032929\n",
       "3899               come  0.027545\n",
       "7639                fly  0.026115\n",
       "5      more_than_6ratio  0.024185\n",
       "22339             which  0.024103\n",
       "9567             honest  0.023404\n",
       "1      more_than_2ratio  0.020367\n",
       "6401                eld  0.016603\n",
       "22647             would  0.015323\n",
       "18480              sing  0.015092\n",
       "13930                no  0.014669\n",
       "5979            downton  0.014647\n",
       "634             america  0.014206\n",
       "20014              talk  0.014157\n",
       "18487           singing  0.014139\n",
       "9536               home  0.014074\n",
       "6253               east  0.013856\n",
       "20840            travel  0.013303\n",
       "20377             thief  0.012161\n",
       "10797               job  0.012094\n",
       "18293             shoot  0.012007\n",
       "4990               date  0.011787\n",
       "6404           elderman  0.011414\n",
       "19049             spend  0.011264\n",
       "2792                bye  0.011264\n",
       "5383            destiny  0.011252\n",
       "22396               why  0.010121\n",
       "5804                doc  0.009638\n",
       "7226              favor  0.009354\n",
       "22188              weak  0.008964\n",
       "22613              work  0.008772\n",
       "19870              swim  0.008596\n",
       "3308          charmaine  0.008465\n",
       "20527              till  0.007600\n",
       "6482                 em  0.007375\n",
       "5127               deep  0.007319\n",
       "9125              harry  0.006964\n",
       "11132              kill  0.006929\n",
       "3492               chop  0.006920\n",
       "1707                bed  0.006655\n",
       "22255              well  0.006442\n",
       "18879          somebody  0.006414\n",
       "21177                um  0.006318\n",
       "13022              mine  0.006198\n",
       "11111               kid  0.006177\n",
       "9194               head  0.005670\n",
       "18673              slow  0.005622\n",
       "3144                 cb  0.005582"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = rc.best_estimator_[0].get_feature_names_out()\n",
    "feature_importances = rc.best_estimator_[-1].feature_importances_\n",
    "fi = pd.DataFrame({'feature':feature_names, 'weight': feature_importances})\n",
    "fi.feature = fi.feature.str[5:]\n",
    "# fi['type'] = fi.feature.str[:3]\n",
    "\n",
    "fi.sort_values(by='weight', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a82ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
